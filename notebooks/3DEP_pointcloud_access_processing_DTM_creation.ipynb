{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "turkish-orchestra",
   "metadata": {
    "VoilaConfiguration": {
     "show_tracebacks": true
    }
   },
   "source": [
    "# Programmatically accessing, processing, and visualizing USGS 3D Elevation Program (3DEP) data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4db3b420",
   "metadata": {},
   "source": [
    "<h2>Table of Contents<span class=\"tocSkip\"></span></h2>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Authors\" data-toc-modified-id=\"Authors-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Authors</a></span></li><li><span><a href=\"#Purpose\" data-toc-modified-id=\"Purpose-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Purpose</a></span></li><li><span><a href=\"#Funding\" data-toc-modified-id=\"Funding-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Funding</a></span></li><li><span><a href=\"#Keywords\" data-toc-modified-id=\"Keywords-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Keywords</a></span></li><li><span><a href=\"#Citation\" data-toc-modified-id=\"Citation-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Citation</a></span></li><li><span><a href=\"#Setup\" data-toc-modified-id=\"Setup-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Setup</a></span><ul class=\"toc-item\"><li><span><a href=\"#Library-Imports\" data-toc-modified-id=\"Library-Imports-6.1\"><span class=\"toc-item-num\">6.1&nbsp;&nbsp;</span>Library Imports</a></span></li><li><span><a href= \"Define-Functions\" data-toc-modified-id=\"Define-Functions-6.2\"><span class=\"toc-item-num\">6.2&nbsp;&nbsp;</span>Define Functions</a></span></ul><li><span><a href=\"#Data-Access-and-Visualization\" data-toc-modified-id=\"Data-Access-and-Visualization-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>Data Access and Visualization</a></span><ul class=\"toc-item\"><li><span><a href=\"#Access-3DEP-Data\" data-toc-modified-id=\"Access-3DEP-Data-7.1\"><span class=\"toc-item-num\">7.1&nbsp;&nbsp;</span>Access 3DEP Data</a></span></li><li><span><a href=\"#Create-Interactive-Ipyleaflet-Map-And-Define-AOI\" data-toc-modified-id=\"Create-Interactive-Ipyleaflet-Map-And-Define-AOI-7.2\"><span class=\"toc-item-num\">7.2&nbsp;&nbsp;</span>Create Interactive Ipyleaflet Map and Define AOI</a></span></li><li><span><a href=\"Find-Intersecting-3DEP-Polygons\" data-toc-modified-id=\"Find-Interseciting-3DEP-Polygons-7.3\"><span class=\"toc-item-num\">7.3&nbsp;&nbsp;</span>Find Intersecting 3DEP Polygons</a></span></li></ul></li><li><span><a href=\"#Data-Processing\" data-toc-modified-id=\"Data-Processing-8\"><span class=\"toc-item-num\">8&nbsp;&nbsp;</span>Data Processing</a></span><ul class=\"toc-item\"></ul></li><li><span><a href=\"#Conclusion\" data-toc-modified-id=\"Conclusion-9\"><span class=\"toc-item-num\">9&nbsp;&nbsp;</span>Conclusion</a></span></li><li><span><a href=\"#Resources\" data-toc-modified-id=\"Resources-10\"><span class=\"toc-item-num\">10&nbsp;&nbsp;</span>Resources</a></span></li></div>\n",
    "    \n",
    "\n",
    "<h2>Authors</h2>\n",
    "<a id='#Authors'></a>\n",
    "\n",
    "<h2>Purpose</h2>\n",
    "<a id='#Purpose'></a>\n",
    "\n",
    "The United States Geological Survey's 3-D Elevation Program (3DEP)is an ongoing effort to provide high-quality topographic -- light detection and ranging (lidar) -- data for the entire conterminous United States, Hawaii, the U.S territories, and Interferometric Synthetic Aperture Radar (IfSAR) for Alaska over an 8-year period. Data acquisition is ongoing, with over 1800 3DEP datasets consisting of >42 trillion points covering an area of > 6 million $km^{2}$ already available for use.\n",
    "\n",
    "3DEP point cloud data can be downloaded on the web from a variety of resources including the <a href=\"https://portal.opentopography.org/datasets\">OpenTopography Portal</a> (restricted to academic users) and the <a href=\"https://prd-tnm.s3.amazonaws.com/LidarExplorer/index.html#/process\">USGS Lidar Explorer</a>. Due to limited computational resources, these web services implement limits on the spatial extent over which point cloud data may be accessed for a single job. In some cases, however, it may benefit users seeking raw, uprocessed point cloud data over large extents to be able to programmatically access, process, and visualize 3DEP point cloud data from a specific area of interest on their local workstation without the web service limitations. \n",
    "\n",
    "This Jupyter notebook is one of two developed focusing on enhancing access and usability of 3DEP data and derivative products. This notebook provides functions, pipelines, and overall workflow for effectively and efficiently accessing documents a Python-based workflow for accessing, processing, and visualizing 3DEP data, leveraging available APIs and cloud resources.\n",
    "\n",
    "<h4>Specific features of this notebook</h4>\n",
    "\n",
    "- Users may (1) select an area of interest (AOI) directly in an interactive map within this notebook; (2) import a shapefile or geoJSON file; or (3) directly input coordinate boundaries for their region of interest.\n",
    "\n",
    "- The relevant 3DEP dataset is accessed via the Amazon Web Services (AWS) API and point cloud data (stored as Entwine tiles) are accessed programmatically, clipped to the user-defined AOI and made available for subsequent processing.\n",
    "\n",
    "- Flexibile and customizable PDAL pipelines are available for classifying, processing, and computing derivate products based on user-defined specifications. \n",
    "\n",
    "- Point cloud data and/or derivative products are saved to local directory or local path defined by user.\n",
    "\n",
    "<h4>Additional resources</h4>\n",
    "An additional Python-based workflow has been developed and is availble in Jupyter Notebook format for <a href=\"http://localhost:8888/notebooks/Programmatically_accessing_3DEP_data_using_USGS_7.5_Quadrangles.ipynb\"> accessing 3DEP point cloud data for specific USGS 7.5' quadrangle footprints</a>.\n",
    "\n",
    "<h2>Funding</h2>\n",
    "<a id='#Funding'></a>\n",
    "OpenTopography is supported by the National Science Foundation under Award Numbers 1948997, 1948994 & 1948857. Funding for 3DEP workflow developement entitled \"Enhancing usability of 3DEP data and web services with Jupyter notebooks\"  was provided through the Community for Data Integration (CDI).\n",
    "\n",
    "<h2>Keywords</h2>\n",
    "<a id='#Keywords'></a>\n",
    "\n",
    "keywords=[\"OpenTopography\",\"USGS\", \"3DEP\", \"PDAL\"]\n",
    "\n",
    "<h2>Citation</h2>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f83dbc2a",
   "metadata": {},
   "source": [
    "## 6. Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25e20850",
   "metadata": {},
   "source": [
    "### 6.1 Getting Started\n",
    "This notebook provides a self-contained workflow for accessing, processing, and visualizing 3DEP point cloud data. There are two options for performing this workflow:\n",
    "\n",
    "1. Download this Jupyter notebook (.ipynb file) to your local file system. \n",
    "    - Create a virtual environement containing the required dependencies (see below).\n",
    "    - Run Juypter notebook on local machine.\n",
    "    - Data download limits will be dependent on user's available hard drive storage. \n",
    "    \n",
    "2. Launch the interactive Jupyter notebook on Google Colaboratory.\n",
    "    - Does not require creation of a virtual environment or installation on local filesystem.\n",
    "    - Requires Google account and access to personal Google Drive folder.\n",
    "    - Data download limits will be dependent on user's available Google Drive storage. \n",
    "    - If you wish to run this notebook in Google Colaboratory click the 'Open in Colab' badge below. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff4d6904",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/cmspeed/OT_3DEP_Workflows/blob/main/notebooks/3DEP_pointcloud_access_processing_DTM_creation.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pleasant-vampire",
   "metadata": {},
   "source": [
    "<h3> 6.1 Library Imports</h3>\n",
    "<a id='#Library-Imports-6.1'></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d18ea4a",
   "metadata": {},
   "source": [
    "### (Optional) -  Install and Run Notebook in Google Colaboratory Environment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4ae9721",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Exectute this block of cells if running on Google Colab\n",
    "### PDAL install in Google Colab is easiest in Conda Environment\n",
    "!git clone https://github.com/cmspeed/OT_3DEP_Workflows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c30ae1fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Installs Conda, which is the easiest way to install PDAL in the Google Colab environment\n",
    "### This will trigger a notification that the \"Your session crashed for an unknown reason.\" \n",
    "### This is normal. Execute next cell.\n",
    "!pip install -q condacolab\n",
    "import condacolab\n",
    "condacolab.install()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5358267",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Import Conda and install PDAL ---- Takes 1-2 minutes\n",
    "import condacolab\n",
    "condacolab.check()\n",
    "!mamba install -q python-pdal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb72a439",
   "metadata": {},
   "outputs": [],
   "source": [
    "########## Runtime must be restarted after the above executes before import of PDAL will be successful!\n",
    "########## On Google Colab ribbon click \"Runtime\" > \"Restart Runtime\".  \n",
    "########## Do not execute any of the previous cells. Execute the following cells only.             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4edf02de",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Install the remaining dependencies\n",
    "!pip install -r OT_3DEP_Workflows/requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ca267a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6173387b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Import Packages (Google Colab Only)\n",
    "import os\n",
    "import json\n",
    "import pdal\n",
    "import gdal\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import ipyleaflet\n",
    "import ipywidgets as widgets\n",
    "from shapely.geometry import shape, Point, Polygon\n",
    "from shapely.ops import transform\n",
    "import requests\n",
    "import pyproj\n",
    "from pyproj.aoi import AreaOfInterest\n",
    "from pyproj.database import query_utm_crs_info\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a27348c",
   "metadata": {},
   "source": [
    "**If running Google Colab, proceed to 6.2 Define Functions, and do not run next cell (6.1.2)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c9525c4",
   "metadata": {},
   "source": [
    "#### 6.1.2.  Option 2 - Install and Run Notebook on Local Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "champion-questionnaire",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "### Import Packages (Local Machine Only)\n",
    "import os\n",
    "import json\n",
    "import pdal\n",
    "from osgeo import gdal\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import ipyleaflet\n",
    "import ipywidgets as widgets\n",
    "from shapely.geometry import shape, Point, Polygon\n",
    "from shapely.ops import transform\n",
    "import requests\n",
    "import pyproj\n",
    "from pyproj.aoi import AreaOfInterest\n",
    "from pyproj.database import query_utm_crs_info\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1299e5e",
   "metadata": {},
   "source": [
    "<h3>6.2 Define Functions</h3>\n",
    "<a id='#Define-Functions-6.2'></a>\n",
    "\n",
    "The functions below are used throughout the rest of the notebook and are included here for clarity.\n",
    "\n",
    "```gcs_to_proj()``` is a function to project from geographic coordinates, WGS84 (EPSG: 4326), to Web Mercator projection (EPSG: 3857).\n",
    "\n",
    "```handle_draw()``` is a function for interactive drawing on ipyleaflet maps and storing the polygon for use in requesting point cloud data.\n",
    "\n",
    "```build_pdal_pipeline()``` is a function used to construct the pdal pipeline to request/process the point cloud data from AWS\n",
    "\n",
    "These functions can be modified as the user sees fit; however, they are designed to work 'out of the box'.\n",
    "\n",
    "**Modify this to reproject from ANY CRS to 3857** <br>\n",
    "**Modify to use | for clarity**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d924c977",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def gcs_to_proj(poly):\n",
    "    \"\"\"\n",
    "    Function for reprojecting polygon shapely object from geographic coordinates (EPSG:4326) \n",
    "    to Web Mercator (EPSG: 3857)). \n",
    "    \n",
    "    :param poly: shapely polygon for user area of interest (AOI)\n",
    "    \"\"\"\n",
    "    wgs84 = pyproj.CRS(\"EPSG:4326\")\n",
    "    web_mercator = pyproj.CRS(\"EPSG:3857\")\n",
    "    project = pyproj.Transformer.from_crs(wgs84, web_mercator, always_xy=True).transform\n",
    "    user_poly_proj3857 = transform(project, poly)\n",
    "    return(user_poly_proj3857)\n",
    "\n",
    "def import_shapefile_to_shapely(path):\n",
    "    shapefile_path = path\n",
    "    gdf = gpd.read_file(shapefile_path)\n",
    "    user_shp = gdf.loc[0, 'geometry']\n",
    "    user_shp_epsg3857 = gcs_to_proj(user_shp)\n",
    "    user_AOI = [[user_shp, user_shp_epsg3857]]\n",
    "    return user_AOI\n",
    "    \n",
    "def handle_draw(target, action, geo_json):\n",
    "    \"\"\"\n",
    "    Allow user to draw area of interest (AOI) on interactive ipyleaflet map.\n",
    "    \n",
    "    :param extent_epsg3857: polygon for user-defined AOI\n",
    "    :param usgs_3dep_dataset_name: name of 3DEP dataset which AOI overlaps\n",
    "    :param resolution: The desired resolution of the pointcloud based on the following definition:\n",
    "    \"\"\"\n",
    "        \n",
    "    geom = dict(geo_json['geometry'])\n",
    "    user_poly = shape(geom)\n",
    "    user_poly_proj3857 = gcs_to_proj(user_poly)\n",
    "    print('AOI is valid and has boundaries of ', user_poly_proj3857.bounds)\n",
    "    user_AOI.append((user_poly, user_poly_proj3857))  #for various reasons, we need user AOI in GCS and EPSG 3857\n",
    "\n",
    "def build_pdal_pipeline(extent_epsg3857, usgs_3dep_dataset_name, pc_resolution, filterNoise = False,\n",
    "                        reclassify = True, savePointCloud = True, pc_outName = 'filter_test', pc_outType = 'laz'):\n",
    "\n",
    "    \"\"\"\n",
    "    Build pdal pipeline for requesting, processing, and saving point cloud data.\n",
    "    \n",
    "    Parameters:\n",
    "    :param extent_epsg3857: Polygon for user-defined AOI in Web Mercator projection (EPS:3857)\n",
    "                            usgs_3dep_dataset_name: name of 3DEP dataset which AOI overlaps. Polygon is generated \n",
    "                            either through the 'handle_draw' methor or by inputing their own coordinates.\n",
    "                            This parameter is set automatically when the user-defined AOI is chosen.\n",
    "    :param usgs_3dep_dataset_name: Name of the 3DEP dataset that the data will be obtained. This parameter is set \n",
    "                                automatically when the user-defined AOI is chosen.\n",
    "    :param pc_resolution: The desired resolution of the pointcloud based on the following definition:\n",
    "        \n",
    "                        Source: https://pdal.io/stages/readers.ept.html#readers-ept\n",
    "                            A point resolution limit to select, expressed as a grid cell edge length. \n",
    "                            Units correspond to resource coordinate system units. For example, \n",
    "                            for a coordinate system expressed in meters, a resolution value of 0.1 \n",
    "                            will select points up to a ground resolution of 100 points per square meter.\n",
    "                            The resulting resolution may not be exactly this value: the minimum possible \n",
    "                            resolution that is at least as precise as the requested resolution will be selected. \n",
    "                            Therefore the result may be a bit more precise than requested.\n",
    "                            \n",
    "    :param filterNoise: Option to remove points from USGS Class 7 (noise).\n",
    "    :param savePointCloud: Option to save (or not) the point cloud dataset.\n",
    "    :param pc_outName: Desired name of file on user's local filesystem. If savePointcloud = False, \n",
    "                       this should be outName = ''\n",
    "    :param pc_outType:  Desired file extension. Input must be either 'las' or 'laz'. If a different file type is\n",
    "    :param requested,the user will get error stating \"Extension must be 'las' or 'laz'\". If savePointcloud = False, \n",
    "        this should be outType = ''\n",
    "    :raise Exception: If user passes in argument that is not 'dtm' or 'dsm'\n",
    "    \"\"\"\n",
    "    \n",
    "    url = \"https://s3-us-west-2.amazonaws.com/usgs-lidar-public/{}/ept.json\".format(usgs_3dep_dataset_name)\n",
    "    \n",
    "    ### TODO: Add an option to choose an output CRS\n",
    "    ### TODO: Add option to output shapefile of the bounding box. \n",
    "    ### TODO: Consider restructuring pipeline construction with list comprehension\n",
    "              #pipline = [dict(zip([1],[stage])) for x in range(1,len(stages))], or similar.\n",
    "    ### Important note: smrf cannot be run in streaming mode\n",
    "    ### TODO: Test performance in and out of streaming mode. Could create a execute_pipeline() \n",
    "        ###   that tests whether streaming is possible.\n",
    "        \n",
    "    #this is the basic pipeline which only accesses the 3DEP data. There is an optional\n",
    "    pointcloud_pipeline = {\n",
    "            \"pipeline\": [\n",
    "                \n",
    "                {\n",
    "                    \"type\": \"readers.ept\",\n",
    "                    \"filename\": str(url),\n",
    "                    \"polygon\": str(extent_epsg3857),\n",
    "                    \"requests\": 3,\n",
    "                    \"resolution\": pc_resolution\n",
    "                }\n",
    "            ]\n",
    "    }\n",
    "    \n",
    "    if filterNoise == True:\n",
    "        \n",
    "        filter_stage = {\n",
    "            \"type\":\"filters.range\",\n",
    "            \"limits\":\"Classification![7:7]\"\n",
    "        }\n",
    "        \n",
    "        pointcloud_pipeline['pipeline'].append(filter_stage)\n",
    "    \n",
    "    if reclassify == True:\n",
    "        \n",
    "        remove_classes_stage = {\n",
    "            \"type\":\"filters.assign\",\n",
    "            \"value\":\"Classification = 0\"\n",
    "        }\n",
    "        \n",
    "        classify_ground_stage = {\n",
    "            \"type\":\"filters.smrf\"\n",
    "        }\n",
    "        \n",
    "        reclass_stage = {\n",
    "            \"type\":\"filters.range\",\n",
    "            \"limits\":\"Classification[2:2]\"\n",
    "        }\n",
    "        \n",
    "        pointcloud_pipeline['pipeline'].append(remove_classes_stage)\n",
    "        pointcloud_pipeline['pipeline'].append(classify_ground_stage)\n",
    "        pointcloud_pipeline['pipeline'].append(reclass_stage)\n",
    "        \n",
    "    if savePointCloud == True:\n",
    "        \n",
    "        if pc_outType == 'las':\n",
    "            savePC_stage = {\n",
    "                \"type\": \"writers.las\",\n",
    "                \"filename\": str(pc_outName)+'.'+ str(pc_outType)\n",
    "            }\n",
    "        elif pc_outType == 'laz':    \n",
    "            savePC_stage = {\n",
    "                \"type\": \"writers.las\",\n",
    "                \"compression\": \"laszip\",\n",
    "                \"filename\": str(pc_outName)+'.'+ str(pc_outType)\n",
    "            }\n",
    "        else:\n",
    "            raise Exception(\"pc_outType must be 'las' or 'laz'.\")\n",
    "\n",
    "        pointcloud_pipeline['pipeline'].append(savePC_stage)\n",
    "        \n",
    "    return pointcloud_pipeline\n",
    "\n",
    "\n",
    "def make_DEM_pipeline(extent_epsg3857, usgs_3dep_dataset_name, pc_resolution, dem_resolution,\n",
    "                      filterNoise = True, reclassify = True, savePointCloud = False, pc_outName = 'filter_test', \n",
    "                      pc_outType = 'laz', demType = 'dtm', gridMethod = 'idw', dem_outName = 'dem_test', \n",
    "                      dem_outExt = 'tif', driver = \"GTiff\"):\n",
    "    \n",
    "    \"\"\"\n",
    "    Build pdal pipeline for creating a digital elevation model (DEM) product from the requested point cloud data. The \n",
    "    user must specify whether a digital terrain (bare earth) model (DTM) or digital surface model (DSM) will be created, \n",
    "    the output DTM/DSM resolution, and the gridding method desired. \n",
    "\n",
    "    The `build_pdal_pipeline() method is used to request the data from the Amazon Web Services ept bucket, and the \n",
    "    user may define any processing steps (filtering or reclassifying). The user must also specify whether the point \n",
    "    cloud should be saved or not. Saving the point cloud is not necessary for the generation of the DEM. \n",
    "\n",
    "    \n",
    "    :param extent_epsg3857: Polygon for user-defined AOI in Web Mercator projection (EPS:3857)\n",
    "                         usgs_3dep_dataset_name: name of 3DEP dataset which AOI overlaps. Polygon is generated \n",
    "                         either through the 'handle_draw' methor or by inputing their own coordinates.\n",
    "                         This parameter is set automatically when the user-defined AOI is chosen.\n",
    "    :param usgs_3dep_dataset_name: Name of the 3DEP dataset that the data will be obtained. This parameter is set \n",
    "                                automatically when the user-defined AOI is chosen.\n",
    "    :param pc_resolution: The desired resolution of the pointcloud based on the following definition:\n",
    "\n",
    "                        Source: https://pdal.io/stages/readers.ept.html#readers-ept\n",
    "                            A point resolution limit to select, expressed as a grid cell edge length. \n",
    "                            Units correspond to resource coordinate system units. For example, \n",
    "                            for a coordinate system expressed in meters, a resolution value of 0.1 \n",
    "                            will select points up to a ground resolution of 100 points per square meter.\n",
    "                            The resulting resolution may not be exactly this value: the minimum possible \n",
    "                            resolution that is at least as precise as the requested resolution will be selected. \n",
    "                            Therefore the result may be a bit more precise than requested.\n",
    "\n",
    "    :param dem_resolution: Desired grid size (in meteres) for output raster DEM \n",
    "    :param filterNoise: Option to remove points from USGS Class 7 (noise).\n",
    "    :param savePointCloud: Option to save (or not) the point cloud dataset.\n",
    "    :param pc_outName: Desired name of file on user's local filesystem. If savePointcloud = False, \n",
    "                  this should be outName = ''\n",
    "    :param pc_outType:  Desired file extension. Input must be either 'las' or 'laz'. If a different file type is requested,\n",
    "                  the user will get error stating \"Extension must be 'las' or 'laz'\". If savePointcloud = False, \n",
    "                  this should be outType = ''\n",
    "    :param demType: Type of DEM produced. Input must 'dtm' (digital terrain model) or 'dsm' (digital surface model).\n",
    "    :param gridMethod: Method used. Options are 'min', 'mean', 'max', 'idw'. ELABORATE MORE HERE.\n",
    "    :param dem_outName: Desired name of DEM file on user's local filesystem.\n",
    "    :param dem_outExt: DEM file extension. ELABORATE MORE HERE.\n",
    "    :param driver: Driver used. ELABORATE MORE HERE.\n",
    "    :raise Exception: If user passes in argument that is not 'dtm' or 'dsm'\n",
    "    \"\"\"\n",
    "\n",
    "    dem_pipeline = build_pdal_pipeline(extent_epsg3857, usgs_3dep_dataset_name, pc_resolution,\n",
    "                                              filterNoise, reclassify, savePointCloud, pc_outName, pc_outType)\n",
    "    \n",
    "    if demType == 'dsm':\n",
    "        dem_stage = {\n",
    "                \"type\":\"writers.gdal\",\n",
    "                \"filename\":str(dem_outName)+ '.' + str(dem_outExt),\n",
    "                \"gdaldriver\":driver,\n",
    "                \"nodata\":-9999,\n",
    "                \"output_type\":gridMethod,\n",
    "                \"resolution\":float(dem_resolution),\n",
    "                \"gdalopts\":\"COMPRESS=LZW,TILED=YES,blockxsize=256,blockysize=256,COPY_SRC_OVERVIEWS=YES\"\n",
    "        }\n",
    "    \n",
    "    elif demType == 'dtm':\n",
    "        groundfilter_stage = {\n",
    "                \"type\":\"filters.range\",\n",
    "                \"limits\":\"Classification[2:2]\"\n",
    "        }\n",
    "\n",
    "        dem_pipeline['pipeline'].append(groundfilter_stage)\n",
    "\n",
    "        dem_stage = {\n",
    "                \"type\":\"writers.gdal\",\n",
    "                \"filename\":str(dem_outName)+ '.' + str(dem_outExt),\n",
    "                \"gdaldriver\":driver,\n",
    "                \"nodata\":-9999,\n",
    "                \"output_type\":gridMethod,\n",
    "                \"resolution\":float(dem_resolution),\n",
    "                \"gdalopts\":\"COMPRESS=LZW,TILED=YES,blockxsize=256,blockysize=256,COPY_SRC_OVERVIEWS=YES\"\n",
    "        }\n",
    "    \n",
    "    else:\n",
    "        raise Exception(\"demType must be 'dsm' or 'dtm'.\")\n",
    "        \n",
    "        \n",
    "    dem_pipeline['pipeline'].append(dem_stage)\n",
    "    \n",
    "    return dem_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8433ad51",
   "metadata": {},
   "source": [
    "## 7. Data Access and Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14ecbeb2",
   "metadata": {},
   "source": [
    "<h3> 7.1 Access 3DEP Data Polygons</h3>\n",
    "<a id='#Access-3DEP-Data-7.1'></a>\n",
    "3DEP lidar point cloud data are stored as Entwine tiles on \n",
    "\n",
    "Executing the next two cells will allow you to select a Area of Interest (AOI) for which point cloud data will be retreived. The map includes a Geo\n",
    "JSON file indicating where 3DEP point cloud data are available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4a267308",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "projecting 3DEP polygons to web mercator (EPSG:3857)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████| 1806/1806 [00:08<00:00, 209.78it/s]\n"
     ]
    }
   ],
   "source": [
    "### Get GeoJSON file for 3DEP outlines from URL \n",
    "\n",
    "#this needs to be modified to always make a new pull, even if it exists because it is possible updated at the source.\n",
    "if not os.path.exists('resources.geojson'):\n",
    "    url = 'https://raw.githubusercontent.com/hobuinc/usgs-lidar/master/boundaries/resources.geojson'\n",
    "    r = requests.get(url)\n",
    "    with open('resources.geojson', 'w') as f:\n",
    "        f.write(r.content.decode(\"utf-8\"))\n",
    "\n",
    "with open('resources.geojson', 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "geo_json_3DEP = ipyleaflet.GeoJSON(data=data, style = {'color': 'green', 'opacity':1, \n",
    "                                       'weight':1.9, 'fillOpacity':0.1})\n",
    "\n",
    "with open('resources.geojson', 'r') as f:\n",
    "    df = gpd.read_file(f)\n",
    "\n",
    "projected_geoms = []\n",
    "\n",
    "print('projecting 3DEP polygons to web mercator (EPSG:3857)')\n",
    "for geometry in tqdm(df['geometry']):\n",
    "        projected_geoms.append(gcs_to_proj(geometry))\n",
    "\n",
    "#df['geometry_epsg3857'] = projected_geoms\n",
    "geometries_GCS = df['geometry']\n",
    "geometries_EPSG3857 = gpd.GeoSeries(projected_geoms)\n",
    "#geometries_EPSG3857 = df['geometry_epsg3857']\n",
    "names = df['name']\n",
    "urls = df['url']\n",
    "num_points = df['count']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86389e5e",
   "metadata": {},
   "source": [
    "<h3> 7.2 Create Interactive Ipyleaflet Map and Define AOI</h3>\n",
    "<a id='#Create-Interactive-Ipyleaflet-Map-And-Define-AOI-7.2'></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d744d9d",
   "metadata": {},
   "source": [
    "**TODO - Add a step where the user polygon is saved as geojson or shp.** <br>\n",
    "**TODO - Add four text boxes for custom user coordinates** <br>\n",
    "**TODO - Add a method for reading in a shapefile for bounding box** <br>\n",
    "**TODO - Remove the Circle and coordinate widget from the side bar**\n",
    "**TODO - Add a method for finding which 3DEP dataset is the 'most appropriate' given the coverage (if they are overlapping)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ee34c098",
   "metadata": {},
   "outputs": [],
   "source": [
    "#enter shapefile path, if applicable. Otherwise leave as shapefile_path = ''\n",
    "#shapefile_path = '/Users/cole/Dropbox/unavco/project/ot_repo/OT_3DEP_Workflows/shapefiles/test_shp.shp'\n",
    "shapefile_path = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c6f7cc4c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Select an Area of Interest using the tools on the left side of the map or enter your own coordinates below\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b75bd143e416421ba26c376d922e8092",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map(center=[39, -100], controls=(ZoomControl(options=['position', 'zoom_in_text', 'zoom_in_title', 'zoom_out_t…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AOI is valid and has boundaries of  (-11213592.797625978, 4760286.723778802, -11203888.075738112, 4766193.3274771925)\n"
     ]
    }
   ],
   "source": [
    "m = ipyleaflet.Map(\n",
    "    basemap=ipyleaflet.basemaps.Esri.WorldTopoMap,\n",
    "    center=(39, -100),\n",
    "    zoom=3,\n",
    "    crs=ipyleaflet.projections.EPSG3857\n",
    "    )\n",
    "\n",
    "m.add_layer(geo_json_3DEP)  #add 3DEP polygons GeoJSON\n",
    "\n",
    "dc = ipyleaflet.DrawControl(\n",
    "    marker={\"shapeOptions\": {\"color\": \"#0000FF\"}},\n",
    "    rectangle={\"shapeOptions\": {\"color\": \"#0000FF\"}},\n",
    "    circle={\"shapeOptions\": {\"color\": \"#0000FF\"}},\n",
    "    circlemarker={},\n",
    ")\n",
    "\n",
    "if os.path.exists(shapefile_path):\n",
    "    user_AOI = import_shapefile_to_shapely(shapefile_path)\n",
    "    print('shapefile loaded. proceed to next cell')\n",
    "    \n",
    "else:\n",
    "    print('Select an Area of Interest using the tools on the left side of the map or enter your own coordinates below')\n",
    "    user_AOI = []\n",
    "    dc.on_draw(handle_draw)\n",
    "    m.add_control(dc)\n",
    "    display(m)\n",
    "# layout = widgets.Layout(width='max', height='40px') #set width and height\n",
    "\n",
    "\n",
    "# widgets.VBox(\n",
    "#     [m,\n",
    "#      widgets.HBox([\n",
    "#                     widgets.VBox([\n",
    "#                             widgets.BoundedFloatText(\n",
    "#                             value=7.5,\n",
    "#                             min=0,\n",
    "#                             max=10.0,\n",
    "#                             step=0.1,\n",
    "#                             description='Upper Left Coordinate:',\n",
    "#                             disabled=False,\n",
    "#                             layout = layout,\n",
    "#                             style= {'description_width': 'initial'}\n",
    "#                             ),\n",
    "\n",
    "#                             widgets.BoundedFloatText(\n",
    "#                             value=7.5,\n",
    "#                             min=0,\n",
    "#                             max=10.0,\n",
    "#                             step=0.1,\n",
    "#                             description='Lower Left Coordinate:',\n",
    "#                             disabled=False,\n",
    "#                             layout = layout,\n",
    "#                             style= {'description_width': 'initial'}\n",
    "#                         )\n",
    "#                     ]\n",
    "#                     ),\n",
    "#                      widgets.VBox([\n",
    "#                                         widgets.BoundedFloatText(\n",
    "#                                         value=7.5,\n",
    "#                                         min=0,\n",
    "#                                         max=10.0,\n",
    "#                                         step=0.1,\n",
    "#                                         description='Upper Left Coordinate:',\n",
    "#                                         disabled=False,\n",
    "#                                         layout = layout,\n",
    "#                                         style= {'description_width': 'initial'}\n",
    "#                                         ),\n",
    "\n",
    "#                                         widgets.BoundedFloatText(\n",
    "#                                         value=7.5,\n",
    "#                                         min=0,\n",
    "#                                         max=10.0,\n",
    "#                                         step=0.1,\n",
    "#                                         description='Lower Left Coordinate:',\n",
    "#                                         disabled=False,\n",
    "#                                         layout = layout,\n",
    "#                                         style= {'description_width': 'initial'}\n",
    "#                         )\n",
    "#                     ]\n",
    "#                     )\n",
    "#                 ]\n",
    "#              ),\n",
    "#     ]\n",
    "#      )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ff54758f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da8affa81c0244519a05aa0cb88e926f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Map(center=[39.2896, -100.68982849999999], controls=(ZoomControl(options=['position', 'zoom_in_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### this is where we want to zoom into the polygon\n",
    "AOI_GCS = user_AOI[0][0]\n",
    "AOI_EPSG3857 = user_AOI[0][1]\n",
    "centroid =  list(AOI_GCS.centroid.coords)[0]\n",
    "\n",
    "intersecting_polys = []\n",
    "for i,geom in enumerate(geometries_EPSG3857):\n",
    "    if geom.intersects(AOI_EPSG3857):\n",
    "        intersecting_polys.append((names[i], geometries_GCS[i], geometries_EPSG3857[i], urls[i], num_points[i]))\n",
    "\n",
    "m = ipyleaflet.Map(\n",
    "    basemap=ipyleaflet.basemaps.Esri.WorldTopoMap,\n",
    "    center=(centroid[1],centroid[0]),\n",
    "    zoom=12,\n",
    "    )\n",
    "\n",
    "wlayer_3DEP = ipyleaflet.WKTLayer(\n",
    "    wkt_string=intersecting_polys[0][1].wkt, \n",
    "    style={\"color\": \"green\"}, hover_style={\"fillColor\": \"red\"},\n",
    ")\n",
    "\n",
    "wlayer_user = ipyleaflet.WKTLayer(\n",
    "    wkt_string=AOI_GCS.boundary.wkt,\n",
    "    style={\"color\": \"blue\"}\n",
    ")\n",
    "\n",
    "#AOI_EPSG3857_wtk = AOI_EPSG3857.wkt + f\" / EPSG:3857\"\n",
    "AOI_EPSG3857_wtk = AOI_EPSG3857.wkt\n",
    "\n",
    "m.add_layer(wlayer_3DEP)\n",
    "m.add_layer(wlayer_user)\n",
    "\n",
    "num_pts_est = int((AOI_EPSG3857.area/intersecting_polys[0][2].area)*(intersecting_polys[0][4]))\n",
    "area = int(AOI_EPSG3857.area/1e6)\n",
    "usgs_3dep_dataset = intersecting_polys[0][0]\n",
    "\n",
    "user_resolution = widgets.RadioButtons(\n",
    "    options=[\n",
    "        (f'Full - All ~{int(num_pts_est):,} points', 1.0),\n",
    "        (f'High - 2m resolution ~{int(num_pts_est/2):,} points', 2.0),\n",
    "        (f'Mid  - 5m resolution ~{int(num_pts_est/5):,} points', 5.0),\n",
    "        (f'Low  - 10m resolution ~{int(num_pts_est/10):,} points', 10.0)\n",
    "    ],\n",
    "    layout={'width': 'max-content'},\n",
    "    disabled = False,\n",
    ")\n",
    "widgets.VBox(\n",
    "    [m,\n",
    "        widgets.Label(value=f'Full dataset will consist of ~{num_pts_est:,} \\\n",
    "        points from the {usgs_3dep_dataset} 3DEP dataset, or choose desired resolution below.'),\n",
    "        user_resolution\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2990899f",
   "metadata": {},
   "source": [
    "## 8. Data Processing\n",
    "PDAL pipelines are useful ways of processing and manipulating point cloud data and creating derivative products. Below you will need to define the processing and/or derivative products which you desire. The ```build_pdal_pipeline()``` function will then construct the appropriate pipeline designed for the user's specifications. Executing this pipeline will request and perform processing on the point cloud data and provide the final result on the user's filesystem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4f17c46",
   "metadata": {},
   "source": [
    "### 8.1 Construct PDAL Pipeline\n",
    "The PDAL pipeline is constructed using the ```build_pdal_pipeline()``` function. Explanations of input parameters are can be found in this cell. Importantly, the ```outputs``` parameter is where the user may define the processing and outputs that they wish to produce. \n",
    "\n",
    "Options (**TODO: Build function capability to allow user to specify an end-product(s)**)  <br>\n",
    "```makeDTM```: Create a digital terrain model (DTM), also known as a bare-earth model <br>\n",
    "```makeDSM```: Create a digital surface model (DSM) <br>\n",
    "```reclassify```: Reclassify the point cloud (instead of using only the USGS classes).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1555ea11",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dirname = '/Users/cole/Dropbox/unavco/project/ot_repo/OT_3DEP_Workflows/notebooks/resolution_tests/'\n",
    "pointcloud_resolution = user_resolution.value\n",
    "pc_pipeline = build_pdal_pipeline(AOI_EPSG3857_wtk, usgs_3dep_dataset, pointcloud_resolution, filterNoise = True,\n",
    "                                  reclassify = False, savePointCloud = False, pc_outName = dirname+'gunnison_low',                                  pc_outType = 'laz')\n",
    "\n",
    "pc_pipeline = pdal.Pipeline(json.dumps(pc_pipeline))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f47cd16c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\\n  \"pipeline\":\\n  [\\n    {\\n      \"filename\": \"https://s3-us-west-2.amazonaws.com/usgs-lidar-public/USGS_LPC_KS_15CoNorth_A2_2015_LAS_2017/ept.json\",\\n      \"polygon\": \"POLYGON ((-11213592.797625978 4760286.723778802, -11213592.797625978 4766193.3274771925, -11203888.075738112 4766193.3274771925, -11203888.075738112 4760286.723778802, -11213592.797625978 4760286.723778802))\",\\n      \"requests\": \"3\",\\n      \"resolution\": \"10\",\\n      \"tag\": \"readers_ept1\",\\n      \"type\": \"readers.ept\"\\n    },\\n    {\\n      \"inputs\":\\n      [\\n        \"readers_ept1\"\\n      ],\\n      \"limits\": \"Classification![7:7]\",\\n      \"tag\": \"filters_range1\",\\n      \"type\": \"filters.range\"\\n    }\\n  ]\\n}\\n'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pc_pipeline.pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8b0d7cb1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.11 s, sys: 536 ms, total: 4.65 s\n",
      "Wall time: 6.09 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Use streaming mode at 1e6 points at a time. This\n",
    "# helps us conserve memory for pipelines that are streamable\n",
    "# check that with the pipeline.streamable property\n",
    "results = pc_pipeline.execute_streaming(chunk_size=1000000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da107b41",
   "metadata": {},
   "source": [
    "If the user only desires point cloud data, they may stop here. Following is a quick overview showing how ground class may be gridded to create a DTM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd2ed7a5",
   "metadata": {},
   "source": [
    "### 8.2 (Optional) DTM Gridding using USGS Classes\n",
    "The following pipeline reads in a las/laz, retains points corresponding to the current ground classification (ie no reclassification is done), and outputs a DTM geotiff. This pipeline may be built into a function(TODO)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2564000a",
   "metadata": {},
   "source": [
    "### Make DSM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e23c3b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dsm_resolution = 2.0\n",
    "dirname = '/Users/cole/Dropbox/unavco/project/ot_repo/OT_3DEP_Workflows/notebooks/resolution_tests/dems/'\n",
    "dem_pipeline = make_DEM_pipeline(AOI_EPSG3857_wtk, usgs_3dep_dataset, pointcloud_resolution, dsm_resolution,\n",
    "                                 filterNoise = True, reclassify = False,  savePointCloud = False, pc_outName = '', \n",
    "                                 pc_outType = '', demType = 'dsm', gridMethod='idw', \n",
    "                                 dem_outName = dirname+'taos_midres_2m_dsm', dem_outExt = 'tif', driver = \"GTiff\")\n",
    "\n",
    "dem_pipeline = pdal.Pipeline(json.dumps(dem_pipeline))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f749cf45",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "results = dem_pipeline.execute_streaming(chunk_size=1000000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7de2d50",
   "metadata": {},
   "source": [
    "### Make DTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06655269",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dtm_resolution = 2.0\n",
    "dirname = '/Users/cole/Dropbox/unavco/project/ot_repo/OT_3DEP_Workflows/notebooks/resolution_tests/dems/'\n",
    "dem_pipeline = make_DEM_pipeline(AOI_EPSG3857_wtk, usgs_3dep_dataset, pointcloud_resolution, dtm_resolution,\n",
    "                                 filterNoise = True, reclassify = False, savePointCloud = False, pc_outName = '', \n",
    "                                 pc_outType = '', demType = 'dtm', dem_outName = dirname+'taos_midres_2m_dtm', \n",
    "                                 dem_outExt = 'tif', driver = \"GTiff\")\n",
    "\n",
    "dem_pipeline = pdal.Pipeline(json.dumps(dem_pipeline))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f59ee07",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "dem_pipeline.execute_streaming(chunk_size=1000000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f8990d5",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Visualize the DEM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d94819b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dem_outName = dirname+'taos_midres_2m_dsm'\n",
    "dem_outExt = 'tif'\n",
    "filename = dem_outName+'.'+dem_outExt\n",
    "gdal_data = gdal.Open(filename)\n",
    "gdal_band = gdal_data.GetRasterBand(1)\n",
    "nodataval = gdal_band.GetNoDataValue()\n",
    "\n",
    "# convert to a numpy array\n",
    "data_array = gdal_data.ReadAsArray().astype(float)\n",
    "data_array\n",
    "\n",
    "# replace missing values if necessary\n",
    "if np.any(data_array == nodataval):\n",
    "    data_array[data_array == nodataval] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13794fde",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(figsize = (15,15))\n",
    "masked_array = np.ma.array (data_array, mask=np.isnan(data_array))\n",
    "cmap = matplotlib.cm.get_cmap(\"Greys\").copy()\n",
    "cmap.set_bad('white',1.)\n",
    "ax.imshow(masked_array, interpolation='nearest', cmap='viridis');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce3c4c69",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf65c16",
   "metadata": {},
   "source": [
    "## Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb70540d",
   "metadata": {},
   "source": [
    "### Debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca41c320",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def build_pdal_pipeline(extent_epsg3857, usgs_3dep_dataset_name, pc_resolution, filterNoise = False,\n",
    "                        reclassify = True, savePointCloud = True, pc_outName = 'filter_test', pc_outType = 'laz'):\n",
    "\n",
    "    \"\"\"\n",
    "    Build pdal pipeline for requesting, processing, and saving point cloud data.\n",
    "    \n",
    "    Parameters:\n",
    "        extent_epsg3857: Polygon for user-defined AOI in Web Mercator projection (EPS:3857)\n",
    "                         usgs_3dep_dataset_name: name of 3DEP dataset which AOI overlaps. Polygon is generated \n",
    "                         either through the 'handle_draw' methor or by inputing their own coordinates.\n",
    "                         This parameter is set automatically when the user-defined AOI is chosen.\n",
    "        usgs_3dep_dataset_name: Name of the 3DEP dataset that the data will be obtained. This parameter is set \n",
    "                                automatically when the user-defined AOI is chosen.\n",
    "        pc_resolution: The desired resolution of the pointcloud based on the following definition:\n",
    "        \n",
    "                        Source: https://pdal.io/stages/readers.ept.html#readers-ept\n",
    "                            A point resolution limit to select, expressed as a grid cell edge length. \n",
    "                            Units correspond to resource coordinate system units. For example, \n",
    "                            for a coordinate system expressed in meters, a resolution value of 0.1 \n",
    "                            will select points up to a ground resolution of 100 points per square meter.\n",
    "                            The resulting resolution may not be exactly this value: the minimum possible \n",
    "                            resolution that is at least as precise as the requested resolution will be selected. \n",
    "                            Therefore the result may be a bit more precise than requested.\n",
    "                            \n",
    "        filterNoise: Option to remove points from USGS Class 7 (noise).\n",
    "        savePointCloud: Option to save (or not) the point cloud dataset.\n",
    "        pc_outName: Desired name of file on user's local filesystem. If savePointcloud = False, \n",
    "                  this should be outName = ''\n",
    "        pc_outType:  Desired file extension. Input must be either 'las' or 'laz'. If a different file type is\n",
    "        requested,the user will get error stating \"Extension must be 'las' or 'laz'\". If savePointcloud = False, \n",
    "        this should be outType = ''\n",
    "    \"\"\"\n",
    "    \n",
    "    url = \"https://s3-us-west-2.amazonaws.com/usgs-lidar-public/{}/ept.json\".format(usgs_3dep_dataset_name)\n",
    "    \n",
    "    ### TODO: Add a reclassify method / SMRF filter stage. (should this be in the build_pipeline or make_dem?)\n",
    "    ### TODO: Add an option to choose an output CRS\n",
    "    ### TODO: Add option to DEM function to specify the type of gridding method (E.g. Zmin, Zmean, Zmax, Zidw, ect)\n",
    "        #DONE\n",
    "    ### TODO: Add option to output shapefile of the bounding box. \n",
    "    ### Important note: smrf cannot be run in streaming mode\n",
    "    ### TODO: Test performance in and out of streaming mode.\n",
    "    \n",
    "    #this is the basic pipeline which only accesses the 3DEP data. There is an optional\n",
    "    pointcloud_pipeline = {\n",
    "            \"pipeline\": [\n",
    "                \n",
    "                {\n",
    "                    \"type\": \"readers.ept\",\n",
    "                    \"filename\": str(url),\n",
    "                    \"polygon\": str(extent_epsg3857),\n",
    "                    \"requests\": 3,\n",
    "                    \"resolution\": pc_resolution\n",
    "                }\n",
    "            ]\n",
    "    }\n",
    "    \n",
    "    if filterNoise == True:\n",
    "        \n",
    "        filter_stage = {\n",
    "            \"type\":\"filters.range\",\n",
    "            \"limits\":\"Classification![7:7]\"\n",
    "        }\n",
    "        \n",
    "        pointcloud_pipeline['pipeline'].append(filter_stage)\n",
    "    \n",
    "    if reclassify == True:\n",
    "        \n",
    "        remove_classes_stage = {\n",
    "            \"type\":\"filters.assign\",\n",
    "            \"value\":\"Classification = 0\"\n",
    "        }\n",
    "        \n",
    "        classify_ground_stage = {\n",
    "            \"type\":\"filters.smrf\"\n",
    "        }\n",
    "        \n",
    "        reclass_stage = {\n",
    "            \"type\":\"filters.range\",\n",
    "            \"limits\":\"Classification[2:2]\"\n",
    "        }\n",
    "        \n",
    "        pointcloud_pipeline['pipeline'].append(remove_classes_stage)\n",
    "        pointcloud_pipeline['pipeline'].append(classify_ground_stage)\n",
    "        pointcloud_pipeline['pipeline'].append(reclass_stage)\n",
    "        \n",
    "    if savePointCloud == True:\n",
    "        \n",
    "        if pc_outType == 'las':\n",
    "            savePC_stage = {\n",
    "                \"type\": \"writers.las\",\n",
    "                \"filename\": str(pc_outName)+'.'+ str(pc_outType)\n",
    "            }\n",
    "        elif pc_outType == 'laz':    \n",
    "            savePC_stage = {\n",
    "                \"type\": \"writers.las\",\n",
    "                \"compression\": \"laszip\",\n",
    "                \"filename\": str(pc_outName)+'.'+ str(pc_outType)\n",
    "            }\n",
    "        else:\n",
    "            raise Exception(\"pc_outType must be 'las' or 'laz'.\")\n",
    "\n",
    "        pointcloud_pipeline['pipeline'].append(savePC_stage)\n",
    "        \n",
    "    return pointcloud_pipeline\n",
    "\n",
    "\n",
    "def make_DEM_pipeline(extent_epsg3857, usgs_3dep_dataset_name, pc_resolution, dem_resolution,\n",
    "                      filterNoise = True, savePointCloud = False, pc_outName = 'filter_test', pc_outType = 'laz',\n",
    "                      demType = 'dtm', gridMethod = 'idw', dem_outName = 'dem_test', dem_outExt = 'tif', driver = \"GTiff\"):\n",
    "    \n",
    "    \"\"\"\n",
    "    Build pdal pipeline for creating a digital elevation model (DEM) product from the requested point cloud data. The \n",
    "    user must specify whether a digital terrain (bare earth) model (DTM) or digital surface model (DSM) will be created, \n",
    "    the output DTM/DSM resolution, and the gridding method desired. \n",
    "\n",
    "    The `build_pdal_pipeline() method is used to request the data from the Amazon Web Services ept bucket, and the \n",
    "    user may define any processing steps (filtering or reclassifying). The user must also specify whether the point \n",
    "    cloud should be saved or not. Saving the point cloud is not necessary for the generation of the DEM. \n",
    "\n",
    "    Parameters:\n",
    "        extent_epsg3857: Polygon for user-defined AOI in Web Mercator projection (EPS:3857)\n",
    "                         usgs_3dep_dataset_name: name of 3DEP dataset which AOI overlaps. Polygon is generated \n",
    "                         either through the 'handle_draw' methor or by inputing their own coordinates.\n",
    "                         This parameter is set automatically when the user-defined AOI is chosen.\n",
    "        usgs_3dep_dataset_name: Name of the 3DEP dataset that the data will be obtained. This parameter is set \n",
    "                                automatically when the user-defined AOI is chosen.\n",
    "        pc_resolution: The desired resolution of the pointcloud based on the following definition:\n",
    "\n",
    "                        Source: https://pdal.io/stages/readers.ept.html#readers-ept\n",
    "                            A point resolution limit to select, expressed as a grid cell edge length. \n",
    "                            Units correspond to resource coordinate system units. For example, \n",
    "                            for a coordinate system expressed in meters, a resolution value of 0.1 \n",
    "                            will select points up to a ground resolution of 100 points per square meter.\n",
    "                            The resulting resolution may not be exactly this value: the minimum possible \n",
    "                            resolution that is at least as precise as the requested resolution will be selected. \n",
    "                            Therefore the result may be a bit more precise than requested.\n",
    "\n",
    "        dem_resolution: Desired grid size (in meteres) for output raster DEM \n",
    "        filterNoise: Option to remove points from USGS Class 7 (noise).\n",
    "        savePointCloud: Option to save (or not) the point cloud dataset.\n",
    "        pc_outName: Desired name of file on user's local filesystem. If savePointcloud = False, \n",
    "                  this should be outName = ''\n",
    "        pc_outType:  Desired file extension. Input must be either 'las' or 'laz'. If a different file type is requested,\n",
    "                  the user will get error stating \"Extension must be 'las' or 'laz'\". If savePointcloud = False, \n",
    "                  this should be outType = ''\n",
    "        demType: Type of DEM produced. Input must 'dtm' (digital terrain model) or 'dsm' (digital surface model).\n",
    "        gridMethod: Method used. Options are 'min', 'mean', 'max', 'idw'. ELABORATE MORE HERE.\n",
    "        dem_outName: Desired name of DEM file on user's local filesystem.\n",
    "        dem_outExt: DEM file extension. ELABORATE MORE HERE.\n",
    "        driver: Driver used. ELABORATE MORE HERE.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    dem_pipeline = build_pdal_pipeline(extent_epsg3857, usgs_3dep_dataset_name, pc_resolution,\n",
    "                                              filterNoise, savePointCloud, pc_outName, pc_outType)\n",
    "    \n",
    "    if demType == 'dsm':\n",
    "        dem_stage = {\n",
    "                \"type\":\"writers.gdal\",\n",
    "                \"filename\":str(dem_outName)+ '.' + str(dem_outExt),\n",
    "                \"gdaldriver\":driver,\n",
    "                \"nodata\":-9999,\n",
    "                \"output_type\":gridMethod,\n",
    "                \"resolution\":float(dem_resolution),\n",
    "                \"gdalopts\":\"COMPRESS=LZW,TILED=YES,blockxsize=256,blockysize=256,COPY_SRC_OVERVIEWS=YES\"\n",
    "        }\n",
    "    \n",
    "    elif demType == 'dtm':\n",
    "        groundfilter_stage = {\n",
    "                \"type\":\"filters.range\",\n",
    "                \"limits\":\"Classification[2:2]\"\n",
    "        }\n",
    "\n",
    "        dem_pipeline['pipeline'].append(groundfilter_stage)\n",
    "\n",
    "        dem_stage = {\n",
    "                \"type\":\"writers.gdal\",\n",
    "                \"filename\":str(dem_outName)+ '.' + str(dem_outExt),\n",
    "                \"gdaldriver\":driver,\n",
    "                \"nodata\":-9999,\n",
    "                \"output_type\":gridMethod,\n",
    "                \"resolution\":float(dem_resolution),\n",
    "                \"gdalopts\":\"COMPRESS=LZW,TILED=YES,blockxsize=256,blockysize=256,COPY_SRC_OVERVIEWS=YES\"\n",
    "        }\n",
    "    \n",
    "    else:\n",
    "        raise Exception(\"demType must be 'dsm' or 'dtm'.\")\n",
    "        \n",
    "        \n",
    "    dem_pipeline['pipeline'].append(dem_stage)\n",
    "        \n",
    "    return dem_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be4d5f42",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pointcloud_resolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba50556e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pointcloud_resolution = user_resolution.value\n",
    "pipeline = build_pdal_pipeline(AOI_EPSG3857_wtk, usgs_3dep_dataset, pointcloud_resolution, filterNoise = False,\n",
    "                        reclassify = True, savePointCloud = True, pc_outName = 'reclassify_test', pc_outType = 'laz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "950a5e3c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pipeline = pdal.Pipeline(json.dumps(pipeline))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d17dbfce",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pipeline.execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d6d3bd",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dtm_resolution = 5.0\n",
    "dem_pipeline = make_DTM_pipeline(AOI_EPSG3857_wtk, usgs_3dep_dataset, pointcloud_resolution, dtm_resolution,\n",
    "                      filterNoise = True, savePointCloud = False, pc_outName = '', pc_outType = '', \n",
    "                      demType = 'dsm', dem_outName = 'dsm_test', driver = \"GTiff\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5995d7b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dem_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64584730",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dem_pipeline = pdal.Pipeline(json.dumps(dem_pipeline))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb874fbc",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "build_dem = dem_pipeline.execute_streaming(chunk_size=1000000)\n",
    "print(dem_pipeline.log)\n",
    "\n",
    "reclassify_stage =\n",
    "    \n",
    "    {\n",
    "        \"type\":\"filters.assign\"\n",
    "        \"value\":\"Classification = Classification[0]}\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "44c81e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip freeze > requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b42d47da",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pointcloud",
   "language": "python",
   "name": "pointcloud"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
